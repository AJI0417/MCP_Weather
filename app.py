import os

# chainlit
import chainlit as cl

# RAG
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Langchain+MCP
from langchain.tools import tool
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_ollama.chat_models import ChatOllama
from langchain.agents import create_agent


LLM_MODEL = "qwen2.5:7b"
SYSTEM_PROMPT = """
ä½ æ˜¯å°ˆæ¥­çš„æ¨‚åœ’ç‡Ÿé‹æ±ºç­–åŠ©æ‰‹ã€‚ä½ çš„è·è²¬æ˜¯æ ¹æ“šå¤©æ°£ç‹€æ³å’Œç‡Ÿé‹æ‰‹å†Šæä¾›ç²¾ç¢ºçš„ç‡Ÿé‹å»ºè­°ã€‚
**çµ•å°åŸå‰‡ï¼šæ‰€æœ‰å›è¦†å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ (zh-tw)ï¼Œä¸”åš´ç¦æé€ ä»»ä½•å·¥å…·çš„æŸ¥è©¢çµæœã€‚å¿…é ˆä¾è³´å·¥å…·å›å‚³çš„çœŸå¯¦æ•¸æ“šã€‚**

ã€æ„åœ–åˆ†é¡èˆ‡å·¥ä½œæµç¨‹ã€‘
æ¥æ”¶åˆ°ç”¨æˆ¶è¨Šæ¯å¾Œï¼Œè«‹å…ˆåˆ¤æ–·å±¬æ–¼ä»¥ä¸‹å“ªç¨®æƒ…å¢ƒï¼Œä¸¦åš´æ ¼åŸ·è¡Œå°æ‡‰å‹•ä½œï¼š

[æƒ…å¢ƒ Aï¼šæŸ¥è©¢å³æ™‚å¤©æ°£]
- ç‰¹å¾µï¼šç”¨æˆ¶è©¢å•ã€Œç¾åœ¨å¤©æ°£ã€ã€ã€Œä»Šå¤©å¤©æ°£ã€ã€ã€ŒæŸåœ°å€å¤©æ°£ã€ç­‰å³æ™‚ç‹€æ³ã€‚
- å‹•ä½œï¼šåªèƒ½èª¿ç”¨å¤©æ°£æŸ¥è©¢å·¥å…·ï¼ˆWeather Toolï¼‰ã€‚
- å›è¦†æ ¼å¼ï¼šç›´æ¥å›å ±æŸ¥è©¢åˆ°çš„å¤©æ°£è³‡è¨Šï¼Œä¸éœ€æä¾›ç‡Ÿé‹å»ºè­°ã€‚

[æƒ…å¢ƒ Bï¼šæŸ¥è©¢ç‡Ÿé‹è¦å‰‡ï¼ˆå‡è¨­æ€§å•é¡Œï¼‰]
- ç‰¹å¾µï¼šç”¨æˆ¶è©¢å•ã€Œé›¨å¤©æ™‚...ã€ã€ã€Œé¢±é¢¨å¤©...ã€ã€ã€Œå“ªäº›è¨­æ–½è¦é—œé–‰ã€ç­‰ä¸€èˆ¬æ€§è¦å‰‡ã€‚
- å‹•ä½œï¼š**çµ•å°ä¸è¦èª¿ç”¨å¤©æ°£å·¥å…·**ã€‚è«‹ç›´æ¥èª¿ç”¨ `search_knowledge_base` æœå°‹ç›¸é—œè¦å‰‡ã€‚
- å›è¦†æ ¼å¼ï¼šæ ¹æ“šæ‰‹å†Šå…§å®¹ç›´æ¥å›ç­”ï¼Œé‡é»åˆ—å‡ºå—å½±éŸ¿çš„è¨­æ–½èˆ‡è¦ç¯„ã€‚è‹¥æ‰‹å†Šæ²’å¯«è«‹èª å¯¦å‘ŠçŸ¥ã€‚

[æƒ…å¢ƒ Cï¼šç¶œåˆç‡Ÿé‹æ±ºç­–]
- ç‰¹å¾µï¼šç”¨æˆ¶è¦æ±‚ã€Œçµ¦äºˆç›®å‰çš„ç‡Ÿé‹å»ºè­°ã€ã€ã€Œæ ¹æ“šç¾åœ¨å¤©æ°£è©²æ€éº¼åšã€ã€‚
- å‹•ä½œï¼š
  1. å…ˆèª¿ç”¨å¤©æ°£æŸ¥è©¢å·¥å…·å–å¾—å³æ™‚å¤©æ°£ã€‚
  2. æ ¹æ“šå–å¾—çš„å¤©æ°£ç‹€æ³ï¼Œèª¿ç”¨ `search_knowledge_base` æŸ¥è©¢å°æ‡‰çš„ç‡Ÿé‹è¦ç¯„ã€‚
- å›è¦†æ ¼å¼ï¼šåš´æ ¼ä¾ç…§ä¸‹æ–¹ã€æ±ºç­–å ±å‘Šæ ¼å¼ã€‘è¼¸å‡ºã€‚

[æƒ…å¢ƒ Dï¼šç™¼é€æ¨æ’­é€šçŸ¥]
- ç‰¹å¾µï¼šç”¨æˆ¶æ˜ç¢ºæŒ‡ç¤ºã€Œç™¼é€é€šçŸ¥ã€ã€ã€Œæ¨æ’­è¨Šæ¯ã€ã€‚
- å‹•ä½œï¼šèª¿ç”¨ `LINE_Notify` ç›¸é—œå·¥å…·ç™¼é€è¨Šæ¯ã€‚
- å›è¦†æ ¼å¼ï¼šå¿…é ˆç­‰å¾…å·¥å…·åŸ·è¡ŒæˆåŠŸå¾Œï¼Œåƒ…è¼¸å‡ºï¼šã€Œâœ… æˆåŠŸï¼šå·²æˆåŠŸæ¨æ’­ [é€šçŸ¥é¡å‹]ã€ã€‚

ã€æ±ºç­–å ±å‘Šæ ¼å¼ã€‘(åƒ…é™æƒ…å¢ƒ C ä½¿ç”¨)
1. **å¤©æ°£ç‹€æ³**ï¼š(å¡«å…¥å·¥å…·å¯¦éš›æŸ¥è©¢çµæœ)
2. **è¨­æ–½çš„ç‡Ÿé‹ç‹€æ³**ï¼š(æ ¹æ“šçŸ¥è­˜åº«åˆ—å‡ºå—å½±éŸ¿è¨­æ–½)
3. **ç‡Ÿé‹æ±ºç­–èˆ‡å»ºè­°**ï¼š(èªªæ˜æ±ºç­–åŸå› èˆ‡å…·é«”å»ºè­°)
4. **æ¨æ’­å»ºè­°**ï¼šæ‚¨å¯ä»¥æŒ‡ç¤ºæˆ‘ç™¼é€ã€[å¡«å…¥å¤©æ°£é¡å‹]é€šçŸ¥ã€
"""


# ========== RAG ==========
VECTOR_DIR = "./faiss_index"
PDF_file_path = "./æ¨‚åœ’ç‡Ÿé‹æ‰‹å†ŠVer3.pdf"
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

if os.path.exists(VECTOR_DIR):
    print("âœ… è¼‰å…¥æ—¢æœ‰ FAISS index")
    vector_store = FAISS.load_local(
        VECTOR_DIR, embeddings, allow_dangerous_deserialization=True
    )
else:
    print("ğŸ†• é¦–æ¬¡å»ºç«‹ FAISS index")
    loader = PyPDFLoader(PDF_file_path)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,
        chunk_overlap=150,
        add_start_index=True,
    )
    texts = splitter.split_documents(docs)
    vector_store = FAISS.from_documents(texts, embeddings)
    vector_store.save_local(VECTOR_DIR)


@tool
def search_knowledge_base(query: str) -> str:
    """æœç´¢æ¨‚åœ’ç‡Ÿé‹æ‰‹å†ŠçŸ¥è­˜åº«ï¼ŒæŸ¥è©¢ä¸åŒå¤©æ°£æ¢ä»¶ä¸‹çš„è¨­æ–½ç‡Ÿé‹è¦å‰‡å’Œæ±ºç­–å»ºè­°ã€‚"""
    docs = vector_store.similarity_search(query, k=5)

    return "\n\n".join(
        f"ã€ä¾†æº {i+1}ã€‘\n{doc.page_content}" for i, doc in enumerate(docs)
    )


# LLM
model = ChatOllama(
    model=LLM_MODEL,
    temperature=0,
)

# MCP Client
client = MultiServerMCPClient(
    {
        "LINE_Notify": {
            "transport": "streamable-http",
            "url": "http://127.0.0.1:8001/mcp",
        },
        "Weather": {
            "transport": "streamable-http",
            "url": "http://127.0.0.1:8002/mcp",
        },
    }
)


@cl.on_chat_start
async def on_chat_start():
    welcome_message = """
    **æ­¡è¿ä½¿ç”¨æ¨‚åœ’ç‡Ÿé‹æ±ºç­–åŠ©æ‰‹ç³»çµ±**
    æˆ‘å¯ä»¥å”åŠ©æ‚¨ï¼š
    A.æŸ¥è©¢å³æ™‚å¤©æ°£è³‡è¨Š
    B.æ ¹æ“šå¤©æ°£æä¾›è¨­æ–½ç‡Ÿé‹å»ºè­°
    C.æ¨æ’­ç‡Ÿé‹æ±ºç­–é€šçŸ¥

    **ä½¿ç”¨æ–¹å¼ï¼š**
    1. ç›´æ¥è©¢å•å¤©æ°£ç‹€æ³ ä¾‹å¦‚ï¼šã€Œç¾åœ¨çš„å¤©æ°£å¦‚ä½•ï¼Ÿã€
    2. è©¢å•ç‰¹å®šå¤©æ°£çš„ç‡Ÿé‹è¦å‰‡ ä¾‹å¦‚ï¼šã€Œé›¨å¤©æ™‚å“ªäº›è¨­æ–½è¦é—œé–‰ï¼Ÿã€
    3. è«‹æ±‚ç¶œåˆå»ºè­° ä¾‹å¦‚ï¼šã€Œæ ¹æ“šç›®å‰å¤©æ°£ï¼Œçµ¦æˆ‘ç‡Ÿé‹å»ºè­°ã€
    4. æ¨æ’­LINE Notifyé€šçŸ¥
    """
    # å–å¾— tools
    tools = await client.get_tools()
    tools.append(search_knowledge_base)
    # å»ºç«‹ agent
    agent = create_agent(model, tools, system_prompt=SYSTEM_PROMPT)

    # å­˜åˆ° user_session
    cl.user_session.set("agent", agent)
    await cl.Message(content=welcome_message).send()


@cl.on_message
async def on_message(message: cl.Message):

    agent = cl.user_session.get("agent")

    ui_msg = cl.Message(content="")
    await ui_msg.send()

    # èª¿ç”¨ agentï¼ˆä½¿ç”¨ stream é€²è¡Œæµå¼è¼¸å‡ºï¼‰
    async for token, metadata in agent.astream(
        {"messages": [{"role": "user", "content": message.content}]},
        stream_mode="messages",
    ):
        # LLM æ–‡å­—
        if metadata.get("langgraph_node") == "model":
            await ui_msg.stream_token(token.text)

    await ui_msg.update()
